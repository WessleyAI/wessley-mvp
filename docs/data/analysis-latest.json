{
  "timestamp": "2026-02-26T19:10:00+02:00",
  "critical": [
    {
      "title": "Knowledge graph has ZERO relationships — completely flat",
      "detail": "Neo4j contains 1,584 nodes ALL labeled 'Component' with 0 relationships. The entire Make→Model→Year→System→Component hierarchy is missing. The graph is useless for structured queries — it's just a flat list. The enricher is either not running or not creating relationships. top_makes in metrics appear to come from document metadata, not actual graph structure.",
      "fix": "Debug engine/graph/enricher.go — verify it creates relationships. Check if seed.go is being called to bootstrap the hierarchy. Run the enricher manually and check logs. The graph should have Make, Model, Year, System, and Component nodes with proper edges."
    },
    {
      "title": "Embedding pipeline stalled — only 2,360 vectors for 19,967 docs (11.8%)",
      "detail": "History shows 0 new vectors across all recent snapshots despite 403 new docs in the last period. The ingest process is running (PID 40432, 31min CPU) and processing files, but vectors are not being created. Either Ollama embedding is failing silently, or the vector upsert to Qdrant is broken. Qdrant shows indexed_vectors_count=0 which means HNSW index hasn't been built (below 10k threshold, but still concerning).",
      "fix": "Check ingest logs for embedding errors. Verify Ollama nomic-embed-text model responds to embedding requests. Check if transform.go is chunking documents before embedding. The 88.2% gap means RAG quality is severely degraded."
    },
    {
      "title": "Two test suites have build failures",
      "detail": "ifixit: TestExtractFixes redeclared (duplicate function in scraper_test.go:28 and scraper_coverage_test.go:168). nhtsa: scraper_test.go references removed struct fields (MakeName, ModelName, ModelYear, Component) on Complaint type — struct was refactored but tests weren't updated.",
      "fix": "ifixit: rename or remove duplicate TestExtractFixes in scraper_coverage_test.go. nhtsa: update scraper_test.go to use current Complaint struct fields (check nhtsa/types.go for correct field names)."
    }
  ],
  "warnings": [
    {
      "title": "Extreme source imbalance — NHTSA is 96.9% of all documents",
      "detail": "NHTSA: 19,339 (96.9%), iFixit: 550 (2.8%), YouTube: 78 (0.4%), Reddit: 0 (0%). A single-source knowledge base is fragile and biased toward complaint data. NHTSA complaints are useful but lack repair procedures, how-to content, and community knowledge."
    },
    {
      "title": "Reddit scraper running but producing zero posts",
      "detail": "Process is running (PID 40408, 33s CPU over hours) but total_posts=0. Either auth is failing, subreddits aren't configured, or Reddit API is blocking requests. This is a dead process consuming resources."
    },
    {
      "title": "Manuals scraper completely inactive",
      "detail": "Despite having 26+ manufacturer sources coded (Toyota, Honda, Ford, BMW, etc.), discovered=0, downloaded=0, ingested=0. The entire manuals pipeline appears to never have run successfully. This is the highest-value untapped source."
    },
    {
      "title": "No new vehicles being discovered",
      "detail": "recent_vehicles is empty in metrics, new_vehicles is empty in all history entries. The vehicle discovery/tagging pipeline from ingested documents isn't working, which compounds the flat graph problem."
    },
    {
      "title": "Disk usage growing — 722MB for codebase",
      "detail": "/tmp/wessley-mvp at 722MB is large for a Go project. Likely includes git history with data files, built binaries, or vendored dependencies. /tmp/wessley-data at 73MB is fine for now but will grow."
    }
  ],
  "healthy": [
    "Infrastructure: Neo4j 5.x, Qdrant, Ollama all connected and responding (Qdrant query: 1.8ms)",
    "Docker containers: neo4j and qdrant running 25h+ with no restarts",
    "Build: go build ./... passes cleanly (no compilation errors in main code)",
    "Core test suites: api, graph, ingest, rag, scraper, semantic all passing",
    "NHTSA scraper: actively ingesting (392 new docs in last 5min window)",
    "iFixit scraper: actively ingesting (11 new docs in last 5min window)",
    "Ollama: running with nomic-embed-text, 78% CPU indicates active use",
    "Ingest process: running with 206MB memory, actively processing files",
    "Error rate: 23/19,967 = 0.12% — excellent"
  ],
  "suggestions": [
    {
      "title": "Fix and activate the manuals scraper — highest ROI data source",
      "impact": "Owner manuals contain structured, authoritative repair/maintenance info. Could add thousands of high-quality documents with proper vehicle tagging. This is the differentiator vs just having NHTSA complaints.",
      "effort": "med"
    },
    {
      "title": "Debug and fix Reddit scraper authentication",
      "impact": "Reddit has rich community repair knowledge (r/MechanicAdvice, r/Cartalk, etc.). Even 1,000 posts would significantly diversify the knowledge base with real-world troubleshooting.",
      "effort": "low"
    },
    {
      "title": "Add forums scraper sources (forums/ package exists but appears unused)",
      "impact": "Automotive forums (Bob Is The Oil Guy, Toyota Nation, etc.) are gold mines of repair knowledge with vehicle-specific threading.",
      "effort": "med"
    },
    {
      "title": "Implement HNSW index optimization for Qdrant",
      "impact": "indexed_vectors_count=0 means all searches are brute-force. At 2,360 vectors it's fine, but at 50k+ it'll degrade. Set indexing_threshold lower or trigger manual indexing.",
      "effort": "low"
    },
    {
      "title": "Add a dedup layer before ingestion",
      "impact": "With NHTSA bulk imports, duplicates are likely. A content-hash dedup check before embedding would save Ollama compute and storage.",
      "effort": "low"
    },
    {
      "title": "Implement metrics for embedding throughput and latency",
      "impact": "Currently blind to why vectors aren't keeping up with docs. Add per-batch timing and success/fail counters to ingest pipeline.",
      "effort": "low"
    }
  ],
  "bugs": [
    {
      "title": "Duplicate test function TestExtractFixes in iFixit package",
      "file": "cmd/scraper-sources/ifixit/scraper_coverage_test.go",
      "line": 168,
      "detail": "TestExtractFixes is declared in both scraper_test.go:28 and scraper_coverage_test.go:168, causing build failure for the entire test package.",
      "fix": "Rename the coverage test version to TestExtractFixes_Coverage or merge the test cases into one function."
    },
    {
      "title": "NHTSA test uses stale Complaint struct fields",
      "file": "cmd/scraper-sources/nhtsa/scraper_test.go",
      "line": 17,
      "detail": "Test references MakeName, ModelName, ModelYear, Component fields that no longer exist on the Complaint struct (likely renamed during refactor).",
      "fix": "Check nhtsa/types.go for current Complaint field names and update test literals at lines 17-20 and 27-30."
    },
    {
      "title": "Graph enricher not creating relationships",
      "file": "engine/graph/enricher.go",
      "line": 0,
      "detail": "0 relationships in Neo4j despite 1,584 Component nodes and 19,967 ingested docs. The enricher is either not running, not being called from ingest, or failing silently.",
      "fix": "Trace the call path from ingest/ingest.go → graph enricher. Check if relationship creation Cypher queries are executing. Add logging around MERGE/CREATE relationship calls."
    },
    {
      "title": "Vector embedding not keeping pace with document ingestion",
      "file": "engine/ingest/transform.go",
      "line": 0,
      "detail": "19,967 docs but only 2,360 vectors. History shows 403 new docs with 0 new vectors in the latest period. The embedding step is either skipping documents or failing.",
      "fix": "Check if transform.go has a batch size limit or error that causes it to skip embedding. Verify the Ollama embed endpoint is being called for each chunk. Add error logging around the embed+upsert path."
    }
  ],
  "metrics": {
    "total_docs": 19967,
    "total_nodes": 1584,
    "total_vectors": 2360,
    "total_relationships": 0,
    "error_rate": 0.0012,
    "sources_active": 3,
    "sources_dead": 2,
    "embedding_ratio": 0.118,
    "nhtsa_pct": 0.969,
    "docs_last_5min": 403,
    "vectors_last_5min": 0
  },
  "strategy": [
    "IMMEDIATE: Fix the embedding pipeline. Having 88% of documents unembedded means the RAG chatbot is working with ~12% of available knowledge. This is the #1 blocker for product quality.",
    "IMMEDIATE: Fix the graph. Zero relationships = zero structured reasoning. The chatbot can't answer 'what systems does a 2024 Camry have?' or traverse vehicle hierarchies. This cripples the UX.",
    "THIS WEEK: Fix both failing test suites. Broken tests erode confidence and block CI. Both fixes are trivial (<30min).",
    "THIS WEEK: Debug Reddit scraper (likely an auth/config issue). Free source diversity.",
    "NEXT SPRINT: Activate manuals scraper. This is the strategic differentiator — structured OEM content with proper vehicle tagging would transform the knowledge base from 'NHTSA complaints search' to 'automotive intelligence platform'.",
    "ARCHITECTURE: The pipeline has a doc-to-vector ratio problem. Either chunking is too aggressive (few chunks per doc) or most docs aren't reaching the embed stage. Profile this path.",
    "PRODUCT: Consider a data quality score per vehicle. Show users 'we have 45 docs and 120 components for 2024 Camry' vs '2 docs for 2024 Genesis GV70'. Transparency builds trust.",
    "COST: Ollama at 78% CPU is fine for MVP. At scale, consider batched embedding with GPU scheduling or switching to a faster model. nomic-embed-text is good but check if mxbai-embed-large gives better retrieval quality."
  ]
}
