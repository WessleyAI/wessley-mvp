{
  "timestamp": "2026-02-28T18:09:00Z",
  "critical": [],
  "warnings": [
    {
      "title": "Scraper locked to single year (2020) — 5 years of data untouched",
      "detail": "Scraper PID 81608 still running with -nhtsa-year 2020 only. 49,860 NHTSA docs from 2020, but years 2021-2025 completely unscraped. Approaching diminishing returns on a single year.",
      "fix": "Restart scraper with: -nhtsa-year 2020,2021,2022,2023,2024,2025"
    },
    {
      "title": "Reddit scraper still stopped — 0 community knowledge after 3 days",
      "detail": "Reddit is the #1 source of real-world mechanic Q&A (r/MechanicAdvice, r/CarTalk). Zero posts scraped. Major gap in data diversity for end-user diagnostics.",
      "fix": "Enable Reddit scraper targeting r/MechanicAdvice, r/CarTalk, r/AskMechanics"
    },
    {
      "title": "YouTube scraper reports 'running' but 0 documents — phantom scraper",
      "detail": "metrics-latest shows youtube status='running', total_docs=0. Either no API key, no search terms configured, or silently erroring. Running for 3 days with zero output.",
      "fix": "Check YouTube scraper logs. Verify YOUTUBE_API_KEY is set and search terms configured."
    },
    {
      "title": "3 biddings containers still consuming resources — 11-20 hours old",
      "detail": "biddings-neo4j (20h), biddings-qdrant (20h), biddings-postgres (11h). All unused by Wessley. Consuming RAM that could go to Ollama.",
      "fix": "docker stop biddings-neo4j biddings-qdrant biddings-postgres && docker rm biddings-neo4j biddings-qdrant biddings-postgres"
    },
    {
      "title": "HNSW index not built — all queries are brute-force",
      "detail": "indexed_vectors_count=0 with 6,325 points. Qdrant threshold is 10,000. Acceptable for now but approaching the threshold where it matters."
    },
    {
      "title": "Relationship density still low — 0.324 rels/node (target 3-5)",
      "detail": "1,762 relationships for 5,440 nodes. Relationship growth stalled this cycle (only +1 since last analysis). Graph enrichment may have completed its current pass."
    },
    {
      "title": "Code repo ballooned to 822MB — likely metrics history bloat",
      "detail": "Code repo grew from 455MB to 822MB (+367MB, +81%) in 4 hours. Git log shows nothing but 'data: auto-update metrics' commits. metrics-history.json and frequent commits are inflating repo size.",
      "fix": "Consider truncating metrics-history.json to last 24h, or move to a separate data branch. Add .gitattributes for LFS on JSON data files."
    }
  ],
  "healthy": [
    "MAJOR WIN: Embedding pipeline RECOVERED — vectors 3,322→6,325 (+3,003, +90.4%) since last analysis",
    "Embedding ratio now 116.3% (6,325 vectors / 5,440 nodes) — vectors exceed nodes due to chunking. RAG coverage is now COMPLETE.",
    "Previous #1 critical issue (stalled embeddings) is RESOLVED — massive 2,825+178 vector burst at 14:48-14:58",
    "Build: go build ./... — zero errors, clean compilation",
    "Tests: ALL 17 test suites PASS across 29 packages, zero failures",
    "Infrastructure: Neo4j 5.x connected + Qdrant green (6,325 points) + Ollama serving (PID 1048 + runner 79838)",
    "Nodes stable at 5,440 — ManualEntry 2,360 + Component 2,203 + System 265 + ModelYear 232 + VehicleModel 200 + Subsystem 147 + Make 33",
    "Error rate: 23/5440 = 0.42% — excellent",
    "Vehicle coverage: 33 Makes, 200 Models, 232 ModelYears — Toyota 420 docs, Honda 310, Ford 285",
    "Disk data: 145MB (+21MB, +17%) — scraper still actively producing",
    "Qdrant: status=green, optimizer_status=ok, Cosine/768d, 2 segments"
  ],
  "suggestions": [
    {
      "title": "Expand scraper to multi-year (2020-2025)",
      "impact": "5x more NHTSA complaint data. 2020 has 49,860 docs — multiply that across 5 more years. Single flag change.",
      "effort": "low"
    },
    {
      "title": "Kill biddings containers to free RAM",
      "impact": "3 unnecessary containers consuming memory. Now that embeddings are working, free RAM helps maintain throughput.",
      "effort": "low"
    },
    {
      "title": "Fix repo size bloat — truncate metrics history or use data branch",
      "impact": "822MB repo is unsustainable. Will cause slow clones and push failures. Truncate metrics-history.json to 24h rolling window.",
      "effort": "low"
    },
    {
      "title": "Enable Reddit scraper for community repair knowledge",
      "impact": "Adds real-world mechanic Q&A — the most valuable data for diagnostic queries. Currently zero community data.",
      "effort": "med"
    },
    {
      "title": "Debug YouTube scraper — 3 days running with 0 output",
      "impact": "YouTube repair videos (ChrisFix, South Main Auto, etc.) are extremely popular. Transcripts would be high-value RAG content.",
      "effort": "med"
    },
    {
      "title": "Add graph enrichment pass to boost relationship density toward 3+ rels/node",
      "impact": "Current 0.324 rels/node limits graph traversal. Cross-linking components to systems and similar complaints across vehicles would improve RAG context.",
      "effort": "med"
    }
  ],
  "bugs": [
    {
      "title": "YouTube scraper silently producing nothing for 3 days",
      "file": "cmd/scraper-youtube/main.go",
      "line": 0,
      "detail": "Status 'running' with 0 total_docs. No error surfaced in metrics. Silent failure.",
      "fix": "Check logs. Add error reporting to metrics when API calls fail. Verify YOUTUBE_API_KEY and search config."
    },
    {
      "title": "Repo size growing ~100MB/hour from metrics auto-commits",
      "file": "docs/data/metrics-history.json",
      "line": 0,
      "detail": "822MB repo, 367MB growth in 4 hours. Git log is nothing but 'data: auto-update metrics' commits. At this rate, repo will hit 2GB+ by tomorrow.",
      "fix": "Truncate metrics-history.json to rolling 24h window. Consider git-filter-branch to clean existing bloat. Use .gitattributes LFS for large JSON."
    }
  ],
  "metrics": {
    "total_docs": 5440,
    "total_nodes": 5440,
    "total_vectors": 6325,
    "total_relationships": 1762,
    "error_rate": 0.0042,
    "sources_active": 2,
    "sources_configured": 4,
    "embedding_ratio": 1.163,
    "node_types": {
      "ManualEntry": 2360,
      "Component": 2203,
      "System": 265,
      "ModelYear": 232,
      "VehicleModel": 200,
      "Subsystem": 147,
      "Make": 33
    },
    "relationship_types": {
      "DOCUMENTED_IN": 918,
      "HAS_SYSTEM": 265,
      "OF_MODEL": 232,
      "HAS_MODEL": 200,
      "HAS_SUBSYSTEM": 147
    },
    "makes_covered": 33,
    "models_covered": 200,
    "top_make_docs": "Toyota:420",
    "docs_last_6h": 0,
    "vectors_last_6h": 3003,
    "rels_last_6h": 1,
    "disk_data_mb": 145,
    "disk_code_mb": 822
  },
  "strategy": [
    "EMBEDDING CRISIS RESOLVED: Vectors jumped 3,322→6,325 (+90.4%). The massive burst at 14:48 (2,825 vectors) and 14:58 (178 vectors) backfilled everything. RAG is now fully operational with 116% coverage (chunking splits nodes into multiple vectors).",
    "SYSTEM IS NOW DEMO-READY: 5,440 nodes, 6,325 vectors, 1,762 relationships, 33 makes, 200 models. All infrastructure green. Build clean, tests passing. This is a viable automotive knowledge graph.",
    "GROWTH HAS PLATEAUED: Zero new docs in last 6 hours. The 2020 NHTSA data is likely exhausted. Multi-year expansion is now the #1 growth lever.",
    "NEW CONCERN — REPO BLOAT: 822MB repo growing ~100MB/hour from metrics commits. This will break CI/CD and slow clones. Needs immediate attention.",
    "NEXT PRIORITIES: (1) Fix repo bloat, (2) Expand to multi-year scraping, (3) Kill biddings containers, (4) Enable Reddit scraper. Items 1-3 are low-effort, high-impact.",
    "DATA DIVERSITY IS THE BIGGEST GAP: 97% of data is NHTSA complaints + OEM manuals. No community knowledge (Reddit), no video content (YouTube), no forum discussions. For a diagnostic AI, real-world user experiences are crucial."
  ],
  "changes_since_last": [
    "RESOLVED: Embedding pipeline recovered — previous #1 critical issue FIXED",
    "MAJOR: Vectors 3,322→6,325 (+3,003, +90.4%) — massive backfill completed",
    "IMPROVED: Embedding ratio 61.1%→116.3% — now exceeds 100% due to chunking",
    "NEW WARNING: Repo size 455MB→822MB (+81%) — metrics auto-commit bloat",
    "STABLE: Nodes unchanged at 5,440 — growth plateau, 2020 data likely exhausted",
    "STABLE: Relationships 1,761→1,762 (+1) — enrichment pass appears complete",
    "STABLE: Disk data 124MB→145MB (+17%)",
    "STABLE: Scraper still 2020-only, Reddit still stopped, YouTube still phantom",
    "STABLE: Build clean, all 17 test suites pass, infrastructure healthy",
    "STABLE: biddings containers still running (postgres 11h, neo4j/qdrant 20h)",
    "NO NEW CODE: All commits since last analysis are 'data: auto-update metrics'"
  ]
}
