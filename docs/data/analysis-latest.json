{
  "timestamp": "2026-02-27T20:09:00+02:00",
  "critical": [
    {
      "title": "Knowledge graph still has ZERO relationships \u2014 graph is structurally useless",
      "detail": "Neo4j has 4,065 nodes (Component: 1,705 + ManualEntry: 2,360) but 0 relationships. Despite commit c3986e0 claiming to fix 'enricher relationships', and ae17c5a fixing 'enricher pipeline', no edges have been created. The enricher code may be fixed but never re-run against existing data, or it's silently failing at runtime. Without relationships, the graph cannot answer hierarchical queries (make\u2192model\u2192year\u2192system\u2192component).",
      "fix": "Run the enricher manually against all existing nodes. Check enricher logs for errors. Verify the Cypher MERGE statements are actually executing. Consider a one-time migration script to retroactively create relationships for all 4,065 nodes."
    },
    {
      "title": "/tmp/wessley-data/ is EMPTY (0 bytes) \u2014 ingest directory has no files",
      "detail": "The ingest process is running with -dir /tmp/wessley-data but that directory is 0 bytes. Either scraped files are being deleted after processing (good if intentional), or scrapers are not writing to this directory at all. Two separate ingest processes are running (PID 6694 compiled binary + PID 65156 go run) \u2014 this is a conflict risk.",
      "fix": "Verify scrapers write to /tmp/wessley-data/. Kill the duplicate ingest process (keep one). Check if files are consumed-and-deleted by design or if the pipeline is broken."
    },
    {
      "title": "Embedding ratio declining \u2014 2,527 vectors for 4,065 nodes (62%) but 0 new vectors in 2+ hours",
      "detail": "History shows total_docs jumped from 1,705 to 4,065 at 16:18 (ManualEntry nodes added) but vectors stayed at 2,527. No new vectors have been created since at least 10:34 today. The embedding pipeline is stalled despite Ollama being up and connected.",
      "fix": "Check ingest logs at /tmp/wessley-ingest.log for embedding errors. Verify ManualEntry nodes have content that can be embedded. The 2,360 ManualEntry nodes added 0 vectors \u2014 the ingest\u2192embed path for manual entries may not exist."
    }
  ],
  "warnings": [
    {
      "title": "Duplicate ingest processes running \u2014 potential conflict",
      "detail": "PID 6694 (/tmp/ingest --dir /tmp/wessley-data) started at 12:34 and PID 65156 (go run cmd/ingest/main.go -dir /tmp/wessley-data) started at 14:54. Both target the same directory. This can cause double-processing, race conditions on file reads, or duplicate vector upserts.",
      "fix": "Kill PID 6694 (the older compiled binary) and keep the go run instance, or vice versa. Standardize on one launch method."
    },
    {
      "title": "Reddit scraper still producing zero posts",
      "detail": "PID 65162 running since 14:54 with -interval 5m -limit 50, consuming 17MB memory, 0 posts. This has been a dead process across multiple analysis cycles. Likely an auth or config issue that was never resolved.",
      "fix": "Check Reddit API credentials. Verify subreddit config. Check for rate-limiting or 403 errors in scraper output."
    },
    {
      "title": "NHTSA, iFixit, YouTube scrapers all stopped",
      "detail": "Previous analysis showed NHTSA actively ingesting 392 docs/5min. Now all three show status=stopped with total_docs=0. The metrics-latest reports 0 for all scraper totals except manuals (2,360). Either scrapers were intentionally stopped or crashed.",
      "fix": "If intentional (focusing on manuals), fine. If not, restart the scrapers. The pipeline needs active data sources to grow."
    },
    {
      "title": "Total document count dropped dramatically: 19,967 \u2192 4,065",
      "detail": "Previous analysis showed 19,967 docs. Now metrics reports 4,065 (matching Neo4j node count). History shows a -18,386 drop at 17:24 on Feb 26. The metrics collection methodology changed \u2014 it now counts Neo4j nodes instead of cumulative ingested docs. This makes historical comparison unreliable.",
      "fix": "Decide on a canonical 'total documents' metric. Neo4j node count and cumulative ingested count measure different things. Track both separately for clarity."
    }
  ],
  "healthy": [
    "Build: go build ./... passes cleanly \u2014 zero compilation errors",
    "Tests: ALL 18 test suites now PASS (previously ifixit and nhtsa had build failures \u2014 both fixed!)",
    "Infrastructure: Neo4j 5.x and Qdrant both running 8h+ stable, Ollama serving embeddings",
    "Docker containers: neo4j and qdrant healthy, no restarts needed",
    "Qdrant: status=green, optimizer_status=ok, query latency ~1.5ms",
    "Manuals pipeline: 2,360 ManualEntry nodes created \u2014 new data source activated since last analysis",
    "Disk usage improved: 430MB (down from 722MB) for codebase",
    "Error rate: 23/4,065 = 0.57% \u2014 still good",
    "Code quality: vehiclenlp package added, enricher pipeline fixes committed, Reddit test updated",
    "Git activity: active development with bug fixes, features, and data updates"
  ],
  "suggestions": [
    {
      "title": "Run enricher backfill on all 4,065 nodes to create relationships",
      "impact": "Transforms the graph from a flat node dump to a queryable knowledge graph. Enables hierarchical vehicle queries, which is the core product value proposition.",
      "effort": "low"
    },
    {
      "title": "Embed the 2,360 ManualEntry nodes \u2014 they have 0 vectors",
      "impact": "ManualEntry content is the highest-quality data (structured OEM info). Embedding them would increase vector count by ~93% and dramatically improve RAG retrieval for repair/maintenance queries.",
      "effort": "low"
    },
    {
      "title": "Kill duplicate ingest process and standardize launch method",
      "impact": "Eliminates race conditions and resource waste. One ingest process is sufficient.",
      "effort": "low"
    },
    {
      "title": "Restart NHTSA and iFixit scrapers \u2014 they were the primary data sources",
      "impact": "Previously contributed 19,000+ documents. Restarting them with proper embedding would rapidly grow the knowledge base.",
      "effort": "low"
    },
    {
      "title": "Fix Reddit scraper or kill it \u2014 it's been dead across multiple analysis cycles",
      "impact": "Either get community knowledge flowing or free the resources. Current state is pure waste.",
      "effort": "med"
    },
    {
      "title": "Add a startup health check that verifies enricher creates at least 1 relationship",
      "impact": "Would have caught the zero-relationships bug immediately instead of letting it persist across multiple days and analysis cycles.",
      "effort": "low"
    }
  ],
  "bugs": [
    {
      "title": "Enricher produces 0 relationships despite multiple fix commits",
      "file": "engine/graph/enricher.go",
      "line": 0,
      "detail": "Commits c3986e0 and ae17c5a both claim to fix enricher relationships, yet Neo4j still shows 0 edges. The fix either doesn't work, isn't being triggered at runtime, or only applies to new ingestions (not backfilling existing nodes).",
      "fix": "Test the enricher in isolation: feed it a single Component node and verify a relationship is created. Check if it requires specific node properties that existing nodes lack. Add an integration test that asserts relationship count > 0."
    },
    {
      "title": "ManualEntry nodes not being embedded into Qdrant",
      "file": "engine/ingest/transform.go",
      "line": 0,
      "detail": "2,360 ManualEntry nodes were created but vector count only went from 2,360 to 2,527 (+167, likely from Component nodes). The embedding pipeline may not handle ManualEntry-type documents, or ManualEntry content format doesn't match expected input.",
      "fix": "Check if the embed path filters by node type. Verify ManualEntry nodes have a 'content' or 'text' field that the embedder reads. Add ManualEntry to the embedding pipeline if it's excluded."
    },
    {
      "title": "Duplicate ingest processes targeting same directory",
      "file": "cmd/ingest/main.go",
      "line": 0,
      "detail": "Two ingest processes (PID 6694 and 65156) running simultaneously on /tmp/wessley-data. No file locking or dedup mechanism visible.",
      "fix": "Add a PID lockfile check at startup. Kill the stale process."
    }
  ],
  "metrics": {
    "total_docs": 4065,
    "total_nodes": 4065,
    "total_vectors": 2527,
    "total_relationships": 0,
    "error_rate": 0.0057,
    "sources_active": 1,
    "sources_dead": 4,
    "embedding_ratio": 0.622,
    "node_types": {
      "Component": 1705,
      "ManualEntry": 2360
    },
    "docs_last_2h": 0,
    "vectors_last_2h": 0,
    "nhtsa_pct": 0,
    "docs_last_5min": 0,
    "vectors_last_5min": 0
  },
  "strategy": [
    "IMMEDIATE: Run enricher backfill to create relationships for all 4,065 nodes. The graph has been flat since day one \u2014 every analysis has flagged this. Two fix commits have shipped but 0 relationships exist. This needs hands-on debugging, not another commit.",
    "IMMEDIATE: Embed ManualEntry nodes. 2,360 high-quality manual entries sitting unembedded is the biggest quick win for RAG quality. Verify the ingest pipeline handles this node type.",
    "IMMEDIATE: Kill duplicate ingest process (PID 6694). Keep PID 65156 (the go run instance with log tee).",
    "TODAY: Decide on scraper strategy \u2014 NHTSA/iFixit/YouTube are all stopped. If intentional (focusing on manual entries first), document it. If not, restart them.",
    "THIS WEEK: Fix or remove Reddit scraper. It has been flagged as dead in every analysis cycle. Either fix the auth issue or remove the dead process.",
    "ARCHITECTURE: The metrics methodology change (19,967\u21924,065) broke historical continuity. Standardize: 'total_docs' = cumulative ingested, 'total_nodes' = Neo4j count, 'total_vectors' = Qdrant count. Track all three independently.",
    "PROGRESS: Test suites all pass now (was 2 failures). ManualEntry data source is new and valuable. Disk usage improved. Code is actively evolving with good commits. The foundation is solid \u2014 the gaps are in runtime operations (enricher, embedding, scraper management), not code quality.",
    "PRODUCT: With 2,360 manual entries and 1,705 components, you have meaningful coverage of 8 makes. Focus on depth over breadth \u2014 fully enrich Toyota/Honda/Ford (your top 3) before expanding to more makes."
  ],
  "changes_since_last": [
    "IMPROVED: All 18 test suites now pass (was 2 build failures in ifixit and nhtsa)",
    "IMPROVED: Disk usage down from 722MB to 430MB",
    "NEW: ManualEntry node type added \u2014 2,360 entries from manuals scraper (was 0 previously)",
    "NEW: Node count up from 1,584 to 4,065 (+156%)",
    "NEW: vehiclenlp package added for NLP-based vehicle extraction",
    "NEW: Multiple bug fix and enricher pipeline commits landed",
    "DEGRADED: All scrapers stopped (NHTSA was actively ingesting 392 docs/5min, now 0)",
    "DEGRADED: Total tracked docs dropped from 19,967 to 4,065 (metrics methodology change)",
    "DEGRADED: Error rate slightly up from 0.12% to 0.57% (still acceptable)",
    "UNCHANGED: Zero relationships in graph \u2014 persists despite two fix commits",
    "UNCHANGED: Reddit scraper still dead (0 posts)",
    "UNCHANGED: Embedding pipeline stalled (0 new vectors in hours)",
    "UNCHANGED: /tmp/wessley-data/ is empty (was not tracked before)",
    "CONCERN: Two ingest processes running simultaneously (new issue)"
  ]
}