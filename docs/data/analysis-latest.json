{
  "timestamp": "2026-03-01T00:09:00+02:00",
  "critical": [
    {
      "title": "Repo size growing ~100MB/hour — will break CI/CD imminently",
      "detail": "822MB repo, all growth from 'data: auto-update metrics' commits every 5-10 min. At this rate, hits GitHub's 2GB soft limit within 12 hours. Git log shows 20 straight metrics commits with no other work.",
      "fix": "Truncate metrics-history.json to rolling 24h window. Run: cd /tmp/wessley-mvp && git filter-branch to clean history, or start fresh data branch. Add docs/data/*.json to .gitattributes for LFS."
    }
  ],
  "warnings": [
    {
      "title": "Pipeline growth has plateaued — zero new docs in 10+ hours",
      "detail": "Last new document ingested at ~20:18 on Feb 28. The 2020 NHTSA dataset appears exhausted. Without multi-year expansion, the knowledge graph is static."
    },
    {
      "title": "Scraper locked to single year (2020) — 5 years of data untouched",
      "detail": "Scraper PID 81608 running with --nhtsa-year 2020 only. Years 2021-2025 completely unscraped. Current 62,812 NHTSA docs all from one year.",
      "fix": "Restart scraper with: --nhtsa-year 2020,2021,2022,2023,2024,2025"
    },
    {
      "title": "Reddit scraper stopped — zero community knowledge after 3+ days",
      "detail": "Reddit (r/MechanicAdvice, r/CarTalk) is the #1 source of real-world mechanic Q&A. Zero posts scraped. Major gap for diagnostic AI."
    },
    {
      "title": "YouTube scraper phantom — 'running' status, 0 documents for 3+ days",
      "detail": "metrics-latest shows youtube status='running', total_docs=0. Silent failure — no errors surfaced. Either missing API key or misconfigured search terms."
    },
    {
      "title": "3 biddings containers consuming resources unnecessarily",
      "detail": "biddings-neo4j, biddings-qdrant, biddings-postgres all running (11-24h). Unused by Wessley, competing for RAM with Ollama embeddings."
    },
    {
      "title": "Relationship density very low — 0.324 rels/node (target 3-5)",
      "detail": "1,764 relationships for 5,441 nodes. Graph traversal quality suffers. Enrichment appears stalled — only +2 rels since last analysis 6h ago."
    },
    {
      "title": "HNSW index not built — brute-force search on 6,325 vectors",
      "detail": "indexed_vectors_count=0. Qdrant auto-indexes at 10,000 threshold. Acceptable now but approaching the limit where query latency increases."
    }
  ],
  "healthy": [
    "EMBEDDING PIPELINE FULLY RECOVERED — vectors 3,322→6,325 (+90.4%) since previous analysis. Previous #1 critical issue is RESOLVED.",
    "Embedding ratio 116.3% (6,325 vectors / 5,441 nodes) — vectors exceed nodes due to chunking. RAG coverage is COMPLETE.",
    "Build: go build ./... — zero errors, clean compilation across all packages",
    "Tests: ALL 17 test suites PASS, zero failures across 29 packages",
    "Infrastructure: Neo4j 5.x ✓ | Qdrant green (6,325 pts, Cosine/768d) ✓ | Ollama serving (nomic-embed-text) ✓",
    "Knowledge graph: 5,441 nodes — ManualEntry 2,360 + Component 2,203 + System 265 + ModelYear 232 + VehicleModel 200 + Subsystem 148 + Make 33",
    "Vehicle coverage: 33 Makes, 200 Models, 232 ModelYears — Toyota 420 docs, Honda 310, Ford 285",
    "Error rate: 23/5,441 = 0.42% — excellent data quality",
    "Scraper-sources + ingest processes running continuously (PIDs 81608, 59635)",
    "Qdrant: status=green, optimizer_status=ok, 2 segments, zero queued updates"
  ],
  "suggestions": [
    {
      "title": "Fix repo bloat NOW — truncate metrics-history.json to 24h rolling window",
      "impact": "Prevents repo from hitting GitHub limits. 822MB→~50MB potential reduction. Unblocks clean CI/CD.",
      "effort": "low"
    },
    {
      "title": "Expand scraper to 2020-2025 multi-year",
      "impact": "~5x more NHTSA data. Single flag change. Biggest data growth lever available.",
      "effort": "low"
    },
    {
      "title": "Kill biddings containers",
      "impact": "Free RAM for Ollama and ingestion. No downside — containers are unused by Wessley.",
      "effort": "low"
    },
    {
      "title": "Enable Reddit scraper for real-world repair knowledge",
      "impact": "Adds community Q&A — most valuable for diagnostic queries. Currently 0% community data.",
      "effort": "med"
    },
    {
      "title": "Debug YouTube scraper — check API key and logs",
      "impact": "YouTube repair videos (ChrisFix, South Main Auto) are extremely popular. Transcripts are high-value RAG content.",
      "effort": "med"
    },
    {
      "title": "Run graph enrichment pass — cross-link components to complaints across vehicles",
      "impact": "Boost relationship density from 0.324 toward 3+ rels/node. Improves graph-based RAG context significantly.",
      "effort": "med"
    },
    {
      "title": "Add test coverage to untested commands (8 packages with [no test files])",
      "impact": "cmd/ingest, cmd/chat, cmd/backfill, cmd/embed-backfill, cmd/scraper-sources, cmd/scraper-youtube, cmd/snapshot-collector, pkg/ollama all lack tests.",
      "effort": "high"
    }
  ],
  "bugs": [
    {
      "title": "YouTube scraper silently failing for 3+ days",
      "file": "cmd/scraper-youtube/main.go",
      "line": 0,
      "detail": "Status='running' with 0 documents. No error surfaced in metrics. Classic silent failure pattern.",
      "fix": "Add error reporting to metrics pipeline. Check YOUTUBE_API_KEY env var. Add startup validation that logs clearly when API key is missing."
    },
    {
      "title": "Metrics auto-commit inflating repo by ~100MB/hour",
      "file": "docs/data/metrics-history.json",
      "line": 0,
      "detail": "822MB repo, 20+ consecutive 'data: auto-update metrics' commits. Each commit adds the full JSON delta to git objects. Will hit GitHub push limits soon.",
      "fix": "Truncate metrics-history.json to last 288 entries (24h at 5min intervals). Add pre-commit hook to enforce max file size. Consider git-lfs for data files."
    }
  ],
  "metrics": {
    "total_docs": 5441,
    "total_nodes": 5441,
    "total_vectors": 6325,
    "total_relationships": 1764,
    "error_rate": 0.0042,
    "sources_active": 2,
    "sources_configured": 4,
    "embedding_ratio": 1.163,
    "makes_covered": 33,
    "models_covered": 200,
    "model_years_covered": 232,
    "docs_last_6h": 1,
    "vectors_last_6h": 0,
    "rels_last_6h": 2,
    "disk_data_mb": 164,
    "disk_code_mb": 822
  },
  "strategy": [
    "EMBEDDING CRISIS FULLY RESOLVED: The massive vector backfill (3,003 new vectors in one burst) brought RAG coverage to 116%. System is now query-ready.",
    "SYSTEM IS DEMO-READY: 5,441 nodes, 6,325 vectors, 33 makes, 200 models. All infra green. Build+tests clean. This is a viable automotive knowledge graph for investor demos.",
    "GROWTH HAS PLATEAUED: Zero meaningful new docs in 10+ hours. The 2020 NHTSA dataset is exhausted. Multi-year expansion is the only path to growth without new sources.",
    "REPO BLOAT IS THE NEW CRITICAL: 822MB and growing. This is now the #1 operational risk — will break pushes, slow CI, and bloat clones. Fix before anything else.",
    "DATA DIVERSITY REMAINS THE STRATEGIC GAP: 97%+ of data is NHTSA complaints + OEM manuals. No community knowledge (Reddit=0), no video content (YouTube=0). For diagnostic AI, real-world user experiences are crucial differentiators.",
    "NEXT 3 MOVES: (1) Fix repo bloat, (2) Multi-year scraper expansion, (3) Enable Reddit. All achievable in one work session."
  ],
  "changes_since_last": [
    "RESOLVED: Previous #1 critical (stalled embeddings) is now healthy — vectors stable at 6,325",
    "NEW CRITICAL: Repo bloat escalated to critical — 822MB and growing, was 455MB 6h ago",
    "STABLE: Nodes 5,440→5,441 (+1) — growth effectively stopped",
    "STABLE: Relationships 1,762→1,764 (+2) — enrichment stalled",
    "STABLE: Vectors unchanged at 6,325 — no new embeddings needed (backfill complete)",
    "STABLE: Error rate unchanged at 0.42%",
    "DEGRADED: Disk code 455→822MB (+81%) from metrics auto-commits",
    "IMPROVED: Disk data 124→164MB (+32%) — scraper still writing files even if not ingesting new nodes",
    "UNCHANGED: Reddit stopped, YouTube phantom, biddings containers still running",
    "UNCHANGED: All 17 test suites pass, build clean, no code changes"
  ]
}
